library(tm)
library(data.table)
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)

#establecer directorio
setwd("~/rstudio_script")
tweets = fread("tweets.csv")
tweetcorpus=VCorpus(VectorSource(tweets$text))
tweetcorpus <- tm_map(tweetcorpus, content_transformer(tolower))

# eliminar puntuación
tweetcorpus <- tm_map(tweetcorpus, removePunctuation) 
# eliminar números
tweetcorpus <- tm_map(tweetcorpus, removeNumbers)
# eliminar stopwords
tweetcorpus <- tm_map(tweetcorpus, removeWords, stopwords("english"))
tweetcorpus <- tm_map(tweetcorpus, removeWords, stopwords("spanish"))
tweetcorpus <- tm_map(tweetcorpus, removeWords, stopwords("catalan"))

# eliminar palabras comunes que no son de ayuda
tweetcorpus <- tm_map(tweetcorpus, removeWords, c("barcelona"))

# eliminar URLs
removeURL <- function(x) gsub("http[[:alnum:]]*", "", x)
tweetcorpus <- tm_map(tweetcorpus, content_transformer(removeURL))
# eliminar espacios en blanco
tweetcorpus = tm_map(tweetcorpus, stripWhitespace)

#filtrar base de datos por palabras clave
keywords = c("crim","crimen","crime","robo","robado","robatori","furt","denuncia","atraco","hurto","theft","robbery","inseguridad","insecure","insecurity","inseguretat","threat","amenaza","pelea", "fight","baralla","degradado","degradat","delito","delicte","offense","offence","odi","odio","hate","maltrato","maltracte","victima","victim","policia","police","herid","ferit","ferida","arma","weapon","violencia","violence","acos","abuse","agresion","agressio","aggression","disturbio","riot","sirena","bullying","stalking","droga","drugs","drogues","camello","dealer","sobredosis","sospechoso","sospitos","suspicious","detenido","detingut","detention","miedo","incivism","incivic")
filtered_tweetcorpus = tm_filter(tweetcorpus, FUN = function(x) any(grep(paste(keywords, collapse="|"),content(x))))
filtered_tweetcorpus

tweetdtm = DocumentTermMatrix(filtered_tweetcorpus, control=list(wordLengths=c(2, 140)))

ui = unique(tweetdtm$i)
tweetdtm = tweetdtm[ui,]

#establecer los temas principales según probabilidad
tweet_lda = LDA(tweetdtm, k = 3, control = list(seed = 1234))

tweet_topics <- tidy(tweet_lda, matrix = "beta")
tweet_topics

tweet_top_terms <- tweet_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

tweet_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

high = tidy(tweet_lda, matrix = "gamma")
tweet_classifications = high %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  ungroup
  
# averiguar qué tweets han sido incluidos en cada modelo

names(tweet_classifications)[1] = 'id'

labelled_tweets = left_join(tidy(filtered_tweetcorpus), tweet_classifications, by='id')

labelled_tweets_1 = filter(labelled_tweets, topic == 1)
labelled_tweets_2 = filter(labelled_tweets, topic == 2)
labelled_tweets_3 = filter(labelled_tweets, topic == 3)

#Exportar a .Txt
write.table(tidy(filtered_tweetcorpus), "C:/Users/Saray/Documents/rstudio_script/tweetfinal.txt", sep="\t")
